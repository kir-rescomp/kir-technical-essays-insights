{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>       Technical articles about HPC, BioInformatics/Computational Biology,        Scientific Computing and Experimental Methods     </p> <p> Programming  Workshops  HPC  Bioinformatics  Containers</p> Latest Content <ul> <li> <p></p>\ud83c\udf97\ufe0fFeatured Posts<p></p> <p>Hand-picked articles covering important topics in research computing</p> <p></p>View Featured<p></p> </li> <li> <p></p> All Posts<p></p> <p>Browse the complete archive of technical articles and tutorials</p> <p></p>View All Posts <p></p> </li> </ul>"},{"location":"#featured-posts-heading","title":"Featured Posts","text":"<ul> <li> <p> Importance of Unit Testing</p> <p>Unit testing is crucial in software development because it verifies individual code units in  isolation, enabling early bug detection, improved code quality, and greater confidence during refactoring and maintenance</p> <p> Read more</p> <p> Nov 28, 2024 \u00b7  7 min read</p> </li> <li> <p> Version Control with Git: A Technical Primer</p> <p>Why <code>Git</code> matters</p> <p> Read more</p> <p> Nov 15, 2024 \u00b7  5 min read</p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/","title":"Demystifying Compiler Toolchains - Understanding Module Names","text":"<p>When you run <code>module avail</code> on our cluster, you'll see names like:</p> <pre><code>Python/3.11.3-GCCcore-12.3.0\nR/4.3.2-gfbf-2023a\nBLAST+/2.14.1-gompi-2023a\nBiopython/1.83-foss-2023a\nmagma/2.7.2-foss-2023a-CUDA-12.1.1\n</code></pre> <p>What do those cryptic suffixes mean? Why <code>foss</code>? Why <code>intel</code>? And why does it matter? Let's decode the mystery of compiler toolchains.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#what-is-a-compiler-toolchain","title":"What Is a Compiler Toolchain?","text":"<p>Think of a toolchain as a complete construction kit for building software. Just as a carpenter needs a coordinated set of tools\u2014hammer, saw, screws that fit the screwdriver\u2014scientific software needs a coordinated set of components:</p> <ul> <li>Compiler: Translates human-readable code (C, C++, Fortran) into machine instructions</li> <li>Math libraries: Optimized routines for linear algebra, FFTs, etc. (BLAS, LAPACK, FFTW)</li> <li>MPI library: Enables parallel communication across nodes (OpenMPI, Intel MPI)</li> <li>Build tools: Utilities that help compile software</li> </ul> <p>A toolchain bundles these components together, ensuring they work harmoniously. This coordination is critical\u2014mixing incompatible components can lead to crashes, incorrect results, or mysterious errors.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#meet-the-common-toolchains","title":"Meet the Common Toolchains","text":"","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#gcccore-the-minimal-foundation","title":"GCCcore: The Minimal Foundation","text":"<p>GCCcore is the GNU Compiler Collection (GCC) on its own, without MPI or math libraries. You'll see this for:</p> <ul> <li>Basic tools that don't need parallel computing (editors, Python, many utilities)</li> <li>Foundation packages that other software builds upon</li> </ul> <p>Example: <code>Python/3.9.6-GCCcore-11.2.0</code></p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#foss-free-and-open-source-software","title":"foss: Free and Open Source Software","text":"<p>foss (yes, it stands for \"Free and Open Source Software\"!) is the most popular toolchain and includes:</p> <ul> <li>GCC: GNU compilers (gcc, g++, gfortran)</li> <li>OpenMPI: Open-source MPI implementation</li> <li>OpenBLAS: Optimized linear algebra library</li> <li>FFTW: Fast Fourier Transform library</li> <li>ScaLAPACK: Parallel linear algebra</li> </ul> <p>Example: <code>GROMACS/2021.3-foss-2021b</code></p> <p>This is your go-to toolchain for most scientific applications. It's free, well-tested, and works with virtually everything.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#intel-the-performance-option","title":"intel: The Performance Option","text":"<p>The intel toolchain uses Intel's proprietary compilers and libraries:</p> <ul> <li>Intel compilers: icc, icpc, ifort</li> <li>Intel MPI: Intel's MPI implementation</li> <li>Intel MKL: Math Kernel Library (highly optimized BLAS, LAPACK, FFTW)</li> </ul> <p>Example: <code>NumPy/1.21.1-intel-2021a</code></p> <p>Intel tools often produce faster code on Intel processors, but they're commercial software (though free for academic use). Some users prefer them for performance-critical applications.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#gompi-the-middle-ground","title":"gompi: The Middle Ground","text":"<p>gompi combines:</p> <ul> <li>GCC: GNU compilers</li> <li>OpenMPI: Open-source MPI</li> </ul> <p>It's like foss but without the math libraries. Used for MPI-enabled applications that bring their own math libraries or don't need them.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#the-version-mystery-whats-2021b","title":"The Version Mystery: What's 2021b?","text":"<p>You'll notice toolchains have versions like <code>2021a</code> or <code>2021b</code>. These are EasyBuild releases that specify exact versions of all components:</p> <ul> <li><code>foss-2021b</code> might mean: GCC 11.2.0, OpenMPI 4.1.1, OpenBLAS 0.3.18, etc.</li> <li><code>foss-2022a</code> would have newer versions: GCC 11.3.0, OpenMPI 4.1.4, etc.</li> </ul> <p>The letter (a, b) indicates releases within the same year. This naming ensures reproducibility\u2014everyone using <code>foss-2021b</code> gets the exact same environment.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#why-toolchains-matter-the-compatibility-rule","title":"Why Toolchains Matter: The Compatibility Rule","text":"<p>Here's the golden rule: Software built with one toolchain should not be mixed with software from another.</p> <p>Why? Imagine building a house where:</p> <ul> <li>The electrical system uses metric measurements</li> <li>The plumbing uses imperial measurements  </li> <li>The structural beams use a third system</li> </ul> <p>Chaos, right? The same applies to compiled software. Mixing toolchains can cause:</p> <ul> <li>Crashes: Incompatible binary interfaces</li> <li>Wrong results: Different math library implementations</li> <li>Performance issues: Conflicting optimizations</li> <li>Mysterious errors: \"Undefined symbol\" messages</li> </ul>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#reading-module-names-a-practical-guide","title":"Reading Module Names: A Practical Guide","text":"<p>Let's decode a real module name:</p> <pre><code>TensorFlow/2.15.1-foss-2023a-CUDA-12.1.1\n</code></pre> <p>Breaking it down: - TensorFlow: The software package - 2.15.1: TensorFlow version - foss-2023a: Built with the foss 2023a toolchain - CUDA-12.1.1: GPU support with CUDA version 12.1.1</p> <p>When you load this module, you're getting TensorFlow compiled specifically with: - GCC compilers from foss-2021a - OpenMPI for distributed training - Optimized math libraries - CUDA for GPU acceleration</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#practical-advice-choosing-your-toolchain","title":"Practical Advice: Choosing Your Toolchain","text":"<p>For most users, follow these guidelines:</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#starting-a-new-project","title":"Starting a New Project?","text":"<p>Use the latest foss toolchain available. Check what's current:</p> <pre><code>module avail foss\n</code></pre>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#need-maximum-performance","title":"Need Maximum Performance?","text":"<p>Try the intel toolchain if: - Your code is CPU-intensive - You're running on Intel processors - You've exhausted optimization with foss</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#continuing-existing-work","title":"Continuing Existing Work?","text":"<p>Stick with the same toolchain you've been using. Consistency matters more than having the absolute latest version.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#installing-new-software","title":"Installing New Software?","text":"<p>Check what toolchains are available:</p> <pre><code>module avail YourSoftware\n</code></pre> <p>If multiple toolchains are available, prefer matching your other dependencies.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#common-questions","title":"Common Questions","text":"<p>Q: Can I mix Python/3.9.6-GCCcore-11.2.0 with NumPy/1.21.1-foss-2021b? A: Generally yes! Since foss-2021b includes GCCcore-11.2.0, they're compatible. EasyBuild handles these dependencies automatically.</p> <p>Q: Why don't we just use one toolchain for everything? A: Different software has different requirements. Some needs GPU support, some doesn't. Some benefits from Intel optimizations, some doesn't. Flexibility helps us optimize for each use case.</p> <p>Q: I got an \"undefined symbol\" error. Is it a toolchain issue? A: Possibly! Check with <code>module list</code> to see what toolchains are loaded. Look for mismatches like mixing foss and intel modules.</p> <p>Q: How do I know which toolchain to use for my work? A: Start with foss\u2014it works for 95% of cases. If you hit performance bottlenecks later, then experiment with intel.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/demystifying-compiler-toolchains---understanding-module-names/#the-bottom-line","title":"The Bottom Line","text":"<p>Compiler toolchains are like orchestras\u2014individual instruments (compiler, MPI, math libraries) working in harmony to produce beautiful results. The toolchain name in module labels tells you which \"orchestra\" was used to build that software.</p> <p>When in doubt:</p> <ol> <li>Check available versions: <code>module avail YourSoftware</code> ( or <code>module spider</code>)</li> <li>Pick a recent foss toolchain: Most compatible, well-tested</li> <li>Stay consistent: Don't mix toolchains unnecessarily</li> <li>Ask for help: We're here to guide you!</li> </ol> <p>Understanding toolchains won't just prevent errors\u2014it will help you make informed decisions about software optimization and compatibility. You're not just loading modules; you're orchestrating a sophisticated software environment.</p>","tags":["HPC","Linux","Toolchains"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/","title":"Why HPC Runs on Linux -  A Beginner's Guide","text":"<p>If you're new to High-Performance Computing (HPC), one of the first things you'll notice is that nearly every supercomputer and research cluster runs Linux. In fact, as of today, 100% of the world's top 500 supercomputers run on Linux. But why? Let's explore what makes Linux the undisputed champion of scientific computing.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#a-brief-history-born-from-collaboration","title":"A Brief History: Born from Collaboration","text":"<p>Linux began in 1991 when a Finnish student named Linus Torvalds wanted to create a free operating system for his computer. He shared his code with the world, inviting others to improve it. What happened next was remarkable: thousands of developers worldwide contributed, creating something more powerful than any single company could build.</p> <p>This collaborative spirit is what makes Linux special\u2014and it's the same spirit that drives scientific research.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#the-linux-philosophy-do-one-thing-well","title":"The Linux Philosophy: Do One Thing Well","text":"<p>Linux inherits its design principles from Unix, an operating system created in the 1970s. The Unix philosophy can be summarized as:</p> <ol> <li>Write programs that do one thing and do it well</li> <li>Write programs to work together</li> <li>Write programs to handle text streams, because that is a universal interface</li> </ol> <p>What does this mean in practice? Instead of having one giant program that tries to do everything, Linux provides many small, focused tools that can be combined in powerful ways. Think of it like LEGO blocks\u2014simple pieces that connect to build complex structures.</p> <p>For example, you might chain together commands like this:</p> <pre><code>cat data.txt | grep \"result\" | sort | uniq &gt; summary.txt\n</code></pre> <p>Each command does one job: <code>cat</code> reads, <code>grep</code> filters, <code>sort</code> orders, <code>uniq</code> removes duplicates. Together, they create a data processing pipeline. This modular approach is perfect for research workflows where you need flexibility and customization.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#why-hpc-loves-linux-three-key-reasons","title":"Why HPC Loves Linux: Three Key Reasons","text":"","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#1-open-source-culture-scientific-values","title":"1. Open Source Culture = Scientific Values","text":"<p>Science thrives on transparency, peer review, and building on others' work. Linux embodies these same values:</p> <ul> <li>Transparency: You can see exactly how the system works</li> <li>Peer Review: Thousands of developers scrutinize the code</li> <li>Community Knowledge: Solutions are shared openly</li> </ul> <p>When you encounter a problem on a Linux cluster, chances are someone else has solved it and shared the solution. This collaborative ecosystem accelerates research.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#2-command-line-efficiency","title":"2. Command-Line Efficiency","text":"<p>At first, the command line might seem intimidating compared to clicking buttons in a graphical interface. But here's the power: the command line is programmable.</p> <p>Imagine you need to process 10,000 data files. With a graphical interface, you'd click 10,000 times. With Linux commands, you write once:</p> <pre><code>for file in *.dat; do\n    process_data \"$file\" &gt; \"${file%.dat}_result.txt\"\ndone\n</code></pre> <p>This automation is essential when working with the massive datasets common in modern research. Plus, you can save your commands in scripts, creating reproducible workflows\u2014crucial for scientific integrity.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#3-the-scientific-computing-ecosystem","title":"3. The Scientific Computing Ecosystem","text":"<p>Linux has become the foundation for an enormous ecosystem of scientific software:</p> <ul> <li>Compilers: GCC, Intel, NVIDIA HPC SDK</li> <li>Libraries: OpenMPI, CUDA, Python scientific stack</li> <li>Tools: SLURM job scheduler, environment modules, containers</li> </ul> <p>These tools were built for Linux and work best on Linux. When you use HPC, you're tapping into decades of scientific software development, all optimized for this environment.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#what-this-means-for-you","title":"What This Means for You","text":"<p>As you start using our HPC cluster, you're joining a global community that values:</p> <ul> <li>Efficiency: Automate repetitive tasks</li> <li>Reproducibility: Document your workflows in scripts  </li> <li>Collaboration: Share solutions and learn from others</li> <li>Flexibility: Combine tools in creative ways</li> </ul> <p>Don't worry if the command line feels unfamiliar at first. Every expert started as a beginner. The Linux philosophy means you can learn incrementally\u2014start with basic commands, then gradually build more sophisticated workflows.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#getting-started","title":"Getting Started","text":"<p>Here are some gentle first steps:</p> <ol> <li>Log into the cluster and explore: <code>ls</code>, <code>pwd</code>, <code>cd</code></li> <li>View files without editing: <code>cat</code>, <code>less</code>, <code>head</code>, <code>tail</code></li> <li>Learn to search: <code>grep</code> finds text, <code>find</code> locates files</li> <li>Try combining commands with pipes (<code>|</code>)</li> </ol> <p>Remember, Linux is designed to be helpful. Most commands have built-in help\u2014just type <code>man command</code> (like <code>man ls</code>) to see the manual.</p>","tags":["HPC","Linux"]},{"location":"blog/why-hpc-runs-on-linux----a-beginners-guide/#welcome-to-the-community","title":"Welcome to the Community","text":"<p>You're now part of a computing tradition that powers everything from weather prediction to drug discovery to gravitational wave detection. Linux might seem complex at first, but its philosophy\u2014small tools, working together, doing one thing well\u2014makes it the perfect foundation for discovery.</p> <p>Welcome to HPC. Welcome to Linux. Let's compute something amazing.</p>","tags":["HPC","Linux"]},{"location":"blog/migrating-r-packages-between-r-versions/","title":"Migrating R Packages Between R Versions","text":"<p>When upgrading to a new version of R on the HPC cluster, your existing package library will not automatically carry over. This is by design\u2014R packages are typically compiled for specific R versions and may not be compatible across major version changes. While you could manually reinstall packages one-by-one, this approach provides a systematic way to preserve and recreate your package environment.</p> <p>The key challenge in HPC environments is that packages often come from different sources\u2014primarily CRAN (the Comprehensive R Archive Network) and Bioconductor (specialized for bioinformatics packages). These repositories have different installation methods and dependency resolution systems, so it's important to track which packages came from where. The approach below separates your packages by source, ensuring they're reinstalled using the correct method.</p>","tags":["Terminals","Bash"]},{"location":"blog/migrating-r-packages-between-r-versions/#important-considerations","title":"Important Considerations","text":"<ul> <li>Timing: Run the export step (<code>my_packages_by_repo.rds</code> creation) while you still have access to your old R version, before switching to the new version</li> <li>Storage location: Save the <code>my_packages_by_repo.rds</code> file somewhere accessible from your home directory or a shared project space\u2014not in the R library directory itself, which may change between versions</li> <li>Installation time: Depending on the number of packages and their dependencies, reinstallation can take considerable time. Consider running this in a batch job or interactive session with sufficient time allocation</li> <li>Dependencies: Some packages may have system-level dependencies that require specific modules to be loaded on the cluster. If installations fail, check that you have the same modules loaded that you used with the previous R version</li> </ul>","tags":["Terminals","Bash"]},{"location":"blog/migrating-r-packages-between-r-versions/#migration-process","title":"Migration Process","text":"<p>Follow these two steps to migrate your packages:</p>","tags":["Terminals","Bash"]},{"location":"blog/migrating-r-packages-between-r-versions/#step-1-export-your-current-package-list","title":"Step 1: Export Your Current Package List","text":"<p>In your current version of R, run the following to create a record of your installed packages:</p> <pre><code># Get all installed packages with their repository info\nall_packages &lt;- as.data.frame(installed.packages())\n\n# Remove base packages\nall_packages &lt;- all_packages[all_packages$Priority != \"base\" | is.na(all_packages$Priority), ]\n\n# Separate CRAN vs Bioconductor\nbioc_packages &lt;- all_packages[grepl(\"Bioconductor\", all_packages$Repository), \"Package\"]\ncran_packages &lt;- all_packages[grepl(\"CRAN\", all_packages$Repository), \"Package\"]\n\n# Save both lists\nsaveRDS(list(cran = cran_packages, bioc = bioc_packages), \"my_packages_by_repo.rds\")\n</code></pre>","tags":["Terminals","Bash"]},{"location":"blog/migrating-r-packages-between-r-versions/#step-2-reinstall-packages-in-the-new-r-version","title":"Step 2: Reinstall Packages in the New R Version","text":"<p>After switching to the new version of R (e.g., by loading a different module), run the following to reinstall your packages:</p> <pre><code># Load package lists\npkg_lists &lt;- readRDS(\"my_packages_by_repo.rds\")\n\n# Install CRAN packages\ninstall.packages(pkg_lists$cran)\n\n# Install Bioconductor packages\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(pkg_lists$bioc)\n</code></pre> <p>The installation will respect your configured library path (check with <code>.libPaths()</code>) and install packages to your personal library directory. CRAN packages are installed first using the standard <code>install.packages()</code> function, followed by Bioconductor packages using <code>BiocManager</code>, which handles Bioconductor's specific versioning and dependency requirements.</p>","tags":["Terminals","Bash"]},{"location":"blog/dynamic-r-library-paths----version-aware-rprofile-configuration/","title":"Dynamic R Library Paths -  Version-Aware .Rprofile Configuration","text":"<p>The Problem When managing multiple R versions in an HPC environment, hardcoding library paths in your .Rprofile can quickly become a maintenance headache. Consider this common scenario:</p> <pre><code>.libPaths(c(\"~/devel/R/4.3/skylake/\"))\n</code></pre> <p>This works fine\u2014until you switch to R 4.4 or 4.5. Suddenly, you're either loading incompatible packages or manually editing your <code>.Rprofile</code> every time you change R modules.</p>","tags":["Terminals","Bash"]},{"location":"blog/dynamic-r-library-paths----version-aware-rprofile-configuration/#the-solution-version-aware-library-paths","title":"The Solution: Version-Aware Library Paths","text":"<p>Instead of hardcoding the R version, we can make .Rprofile detect the current R version automatically and set the appropriate library path:</p> <pre><code># Dynamically set R package location based on R version\nr_version &lt;- paste(R.version$major, \n                   strsplit(R.version$minor, \"\\\\.\")[[1]][1], \n                   sep = \".\")\nplatform &lt;- R.version$platform\nuser_lib &lt;- path.expand(sprintf(\"~/devel/R/%s-library/%s/\", platform, r_version))\n\n# Check if the directory exists, if not, warn the user\nif (dir.exists(user_lib)) {\n  .libPaths(c(user_lib, .libPaths()))\n} else {\n  warning(sprintf(\"User library path does not exist: %s\", user_lib))\n  # Optionally, create it automatically:\n  # dir.create(user_lib, recursive = TRUE, showWarnings = FALSE)\n}\n</code></pre> <p>How It Works</p> <ul> <li>Extract the version: <code>R.version$major</code> and <code>R.version$minor</code> give us the version components</li> <li>Parse major.minor: We extract just \"4.3\", \"4.4\", or \"4.5\" (not the full minor version like \"4.3.2\")</li> <li>Get the platform: <code>R.version$platform</code> provides the architecture string (e.g., <code>x86_64-pc-linux-gnu</code>)</li> <li>Construct the path: Using <code>sprintf()</code> to build the path dynamically</li> <li>Prepend to library paths: Using <code>.libPaths(c(user_lib, .libPaths()))</code> ensures your user library takes precedence while keeping system libraries as fallback</li> </ul>","tags":["Terminals","Bash"]},{"location":"blog/dynamic-r-library-paths----version-aware-rprofile-configuration/#complete-rprofile-example","title":"Complete <code>.Rprofile</code> Example","text":"<pre><code># Dynamically set R package location based on R version and platform\nr_version &lt;- paste(R.version$major, \n                   strsplit(R.version$minor, \"\\\\.\")[[1]][1], \n                   sep = \".\")\nplatform &lt;- R.version$platform\nuser_lib &lt;- path.expand(sprintf(\"~/devel/R/%s-library/%s/\", platform, r_version))\n\nif (dir.exists(user_lib)) {\n  .libPaths(c(user_lib, .libPaths()))\n} else {\n  warning(sprintf(\"User library path does not exist: %s\", user_lib))\n}\n\n# Set a local mirror for packages\noptions(repos = structure(c(CRAN = \"https://www.stats.bris.ac.uk/R/\")))\n\n# Set cairo as the default bitmap type\noptions(bitmapType = 'cairo')\n</code></pre>","tags":["Terminals","Bash"]},{"location":"blog/dynamic-r-library-paths----version-aware-rprofile-configuration/#alternative-explicit-version-checking","title":"Alternative: Explicit Version Checking","text":"<p>If you need more granular control or want to handle specific versions differently:</p> <pre><code># Get R version and platform\nr_version &lt;- getRversion()\nplatform &lt;- R.version$platform\n\nif (r_version &gt;= \"4.5.0\") {\n  user_lib &lt;- path.expand(sprintf(\"~/devel/R/%s-library/4.5/\", platform))\n} else if (r_version &gt;= \"4.4.0\") {\n  user_lib &lt;- path.expand(sprintf(\"~/devel/R/%s-library/4.4/\", platform))\n} else if (r_version &gt;= \"4.3.0\") {\n  user_lib &lt;- path.expand(sprintf(\"~/devel/R/%s-library/4.3/\", platform))\n} else {\n  user_lib &lt;- NULL\n}\n\nif (!is.null(user_lib) &amp;&amp; dir.exists(user_lib)) {\n  .libPaths(c(user_lib, .libPaths()))\n}\n</code></pre> <p>This approach is useful if different R versions need different handling or if you're maintaining legacy versions with special requirements.</p> <p>Tips</p> <p>Create library directories in advance: Set up your directory structure before installing packages:</p> <pre><code>mkdir -p ~/devel/R/x86_64-pc-linux-gnu-library/{4.3,4.4,4.5}\n</code></pre> <ul> <li>Verify your platform string: Check what R reports:</li> </ul> <pre><code>R.version$platform\n# [1] \"x86_64-pc-linux-gnu\"\n</code></pre> <p>Test after changes: After modifying .Rprofile, start a new R session and verify:</p> <pre><code>.libPaths()\n# Should show your user library first, then system libraries\n</code></pre>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/","title":"Fixing Ghostty Backspace Behaviour Over SSH","text":"<p>If you've recently switched to Ghostty and noticed that backspace doesn't work properly when SSH'ing into remote servers, you're not alone. Instead of deleting characters, pressing backspace might move the cursor around or print control characters like <code>^?</code> or <code>^H</code>. This post explains why this happens and how to fix it.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#the-problem","title":"The Problem","text":"<p>When you SSH into a remote server from Ghostty, the backspace key may not delete characters as expected. This is frustrating when you're trying to edit commands on the remote shell, as each backspace press either does nothing or produces unexpected behaviour.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#why-this-happens","title":"Why This Happens","text":"<p>The issue boils down to a mismatch between what Ghostty sends when you press backspace and what the remote shell expects to receive.</p> <p>When you establish an SSH connection, the remote shell needs to know certain things about your terminal:</p> <ul> <li>What type of terminal you're using (the <code>TERM</code> environment variable)</li> <li>What character code to treat as \"erase\" (configured via <code>stty</code>)</li> <li>Various other terminal capabilities and key sequences</li> </ul> <p>By default, SSH sessions inherit only a basic environment. The remote server doesn't automatically know about Ghostty's preferred terminal settings, including what byte sequence Ghostty sends for backspace. Without this information, the remote shell's line editing configuration (<code>stty erase</code>) doesn't match what Ghostty is actually sending, resulting in the backspace misbehaviour.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#the-solution","title":"The Solution","text":"<p>Ghostty provides a shell integration feature specifically designed to address this. Add the following line to your Ghostty configuration file at <code>~/.config/ghostty/config</code>:</p> <pre><code>shell-integration-features = ssh-env\n</code></pre> <p>This setting instructs Ghostty to export a richer, Ghostty-aware environment when you initiate SSH connections. The remote server will then receive the correct <code>TERM</code> value and related variables, allowing it to configure its line editing to match what Ghostty actually sends for backspace and other special keys.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#complete-example-configuration","title":"Complete Example Configuration","text":"<p>Here's a more complete Ghostty configuration including the SSH environment fix and some other useful settings:</p> <pre><code>font-family = FiraCode Nerd Font Mono SemBd\nfont-size = 11\nbackground-opacity = 0.85\nkeybind = ctrl+v=paste_from_clipboard\nshell-integration-features = ssh-env\n</code></pre>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#the-zellij-complication","title":"The Zellij Complication","text":"<p>If you use Zellij (a terminal multiplexer) with Ghostty, you'll encounter an additional layer of complexity.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#why-zellij-changes-things","title":"Why Zellij Changes Things","text":"<p>When Zellij is running, your keystrokes follow this path:</p> <pre><code>Ghostty \u2192 Zellij \u2192 SSH \u2192 Remote Shell\n</code></pre> <p>Zellij acts as an intermediary terminal layer with its own ideas about terminal compatibility and key handling. From the remote host's perspective, it's no longer talking directly to Ghostty\u2014instead, it sees whatever <code>TERM</code> and terminal behaviour Zellij presents.</p> <p>This means that while <code>shell-integration-features = ssh-env</code> solves the problem for direct SSH connections from Ghostty, it doesn't help when Zellij is in the middle. The remote server needs different terminal metadata when accessed through Zellij.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#the-additional-fix-for-zellij","title":"The Additional Fix for Zellij","text":"<p>To make backspace work correctly when SSH'ing from within Zellij, you need to configure SSH to send explicit terminal information. Add the following to your SSH host configuration in <code>~/.ssh/config</code>:</p> <pre><code>Host your-remote-host\n    SendEnv TERM\n    SetEnv TERM=xterm-256color\n</code></pre> <p>These directives ensure that:</p> <ol> <li><code>SendEnv TERM</code> tells SSH to send the <code>TERM</code> environment variable to the remote host</li> <li><code>SetEnv TERM=xterm-256color</code> explicitly sets a widely-compatible terminal type</li> </ol> <p>This configuration works around the Zellij layer by forcing consistent terminal metadata that the remote host can understand and configure appropriately.</p>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#wildcard-configuration","title":"Wildcard Configuration","text":"<p>If you want this fix to apply to all your SSH connections, you can use a wildcard:</p> <pre><code>Host *\n    SendEnv TERM\n    SetEnv TERM=xterm-256color\n</code></pre>","tags":["Terminals","Bash"]},{"location":"blog/fixing-ghostty-backspace-behaviour-over-ssh/#summary","title":"Summary","text":"<ul> <li>Direct SSH from Ghostty: Enable <code>shell-integration-features = ssh-env</code> in Ghostty's config</li> <li>SSH from Zellij in Ghostty: Additionally configure <code>~/.ssh/config</code> to explicitly set and send the <code>TERM</code> variable</li> </ul> <p>With both configurations in place, your backspace key should work reliably whether you're SSH'ing directly from Ghostty or from within a Zellij session.</p>","tags":["Terminals","Bash"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/","title":"Unit Testing for Research Software - A Practical Guide","text":"<p>Scientific software powers modern research, from analyzing genomic sequences to simulating climate models. Yet much of this software is written by researchers who are self-taught programmers, often under pressure to produce results quickly. Unit testing might seem like an extra burden, but it's actually a fundamental practice that makes your research more reliable, your code easier to maintain, and your findings more reproducible.</p> <p>What is Unit Testing?</p> <p>Unit testing is the practice of writing small, focused tests that verify individual components (or \"units\") of your code work correctly in isolation. Think of it as checking each instrument in your lab before running an experiment, rather than waiting until the end to discover something was miscalibrated. Let's look at a concrete example from computational biology. Suppose you're writing a tool to calculate quality scores from FASTQ files\u2014a common task when processing sequencing data:</p> <p></p><pre><code># seq_quality.py\ndef phred_to_prob(phred_score):\n    \"\"\"Convert Phred quality score to error probability.\"\"\"\n    return 10 ** (-phred_score / 10)\n\ndef average_quality(quality_string):\n    \"\"\"Calculate average Phred score from ASCII quality string.\"\"\"\n    if not quality_string:\n        raise ValueError(\"Quality string cannot be empty\")\n\n    # FASTQ uses ASCII offset of 33\n    scores = [ord(char) - 33 for char in quality_string]\n    return sum(scores) / len(scores)\n</code></pre> Here are the unit tests for these functions:<p></p> <pre><code># test_seq_quality.py\nimport pytest\nfrom seq_quality import phred_to_prob, average_quality\n\ndef test_phred_conversion_perfect_quality():\n    \"\"\"Test Phred score of 40 (99.99% accuracy).\"\"\"\n    assert abs(phred_to_prob(40) - 0.0001) &lt; 1e-6\n\ndef test_phred_conversion_poor_quality():\n    \"\"\"Test Phred score of 10 (90% accuracy).\"\"\"\n    assert abs(phred_to_prob(10) - 0.1) &lt; 1e-6\n\ndef test_average_quality_uniform():\n    \"\"\"Test with uniform quality scores.\"\"\"\n    # 'III' = ASCII 73, Phred = 73-33 = 40\n    result = average_quality(\"III\")\n    assert result == 40.0\n\ndef test_average_quality_mixed():\n    \"\"\"Test with varied quality scores.\"\"\"\n    # '!~' = ASCII 33 and 126, Phred 0 and 93\n    result = average_quality(\"!~\")\n    assert result == 46.5\n\ndef test_average_quality_empty_string():\n    \"\"\"Test error handling for empty input.\"\"\"\n    with pytest.raises(ValueError):\n        average_quality(\"\")\n</code></pre> <p>Each test is independent, runs in milliseconds, and checks one specific behavior. If <code>test_phred_conversion_perfect_quality</code> fails, you know exactly where the problem is.</p>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#why-unit-testing-matters-for-research-software","title":"Why Unit Testing Matters for Research Software","text":"","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#reproducibility","title":"Reproducibility","text":"<p>Research depends on reproducibility, yet computational results can be surprisingly fragile. A subtle bug in your analysis pipeline might go undetected for months, potentially affecting published results. Unit tests provide a safety net: if your tests pass, you have confidence that the core logic hasn't been accidentally broken.</p>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#collaboration-and-reuse","title":"Collaboration and Reuse","text":"<p>Research software is increasingly collaborative. When a colleague contributes code or you return to your own code after six months, unit tests document expected behavior and catch unintended changes. They're executable documentation that never goes out of date.</p>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#complex-environments","title":"Complex Environments","text":"<p>If you work with HPC clusters, you know that software behavior can vary across environments\u2014different library versions, compilers, or container configurations. Unit tests help verify that your code behaves identically whether running on your laptop, in an Apptainer container, or on a compute node with 8 GPUs.</p>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#confidence-to-refactor","title":"Confidence to Refactor","text":"<p>Scientific code often needs optimization. Perhaps your Python script needs to be 10x faster, or you're switching from NumPy to CuPy for GPU acceleration. Unit tests let you refactor aggressively while ensuring correctness isn't sacrificed for performance.</p>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#catching-edge-cases","title":"Catching Edge Cases","text":"<p>Research data is messy. Empty files, negative values where you expected positive, Unicode characters in supposedly ASCII data\u2014unit tests force you to think through these scenarios before they cause a job to fail after running for 48 hours on a cluster.</p>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#core-fundamental-rules","title":"Core Fundamental Rules","text":"<ol> <li>Test One Thing at a Time Each test should verify a single, specific behavior. If a test fails, you should immediately know what went wrong without debugging.</li> </ol> <p>Good:</p> <pre><code>def test_reverse_complement_single_nucleotide():\n    assert reverse_complement(\"A\") == \"T\"\n\ndef test_reverse_complement_sequence():\n    assert reverse_complement(\"ATCG\") == \"CGAT\"\n</code></pre> <p>Bad:</p> <p></p><pre><code>def test_reverse_complement():\n    assert reverse_complement(\"A\") == \"T\"\n    assert reverse_complement(\"ATCG\") == \"CGAT\"\n    assert reverse_complement(\"\") == \"\"\n    # If this fails, which case broke?\n</code></pre> 2. Tests Must Be Independent Tests should not depend on each other or share state. They should run successfully in any order, in parallel, or in isolation.<p></p> <p>Bad:</p> <pre><code># test_analysis.py\nresults = None  # Shared state!\n\ndef test_load_data():\n    global results\n    results = load_dataset(\"data.csv\")\n    assert results is not None\n\ndef test_calculate_mean():\n    # Depends on test_load_data running first!\n    assert calculate_mean(results) &gt; 0\n</code></pre> <ol> <li> <p>Tests Should Be Fast Unit tests should run in milliseconds, not minutes. If you need to test with real data files or database connections, those are integration tests\u2014still valuable, but separate from unit tests. Fast tests mean you'll actually run them frequently.</p> </li> <li> <p>Use Descriptive Names Test names should describe what they're testing and what the expected outcome is. When a test fails in a CI/CD pipeline at 2 AM, good names are invaluable.</p> </li> </ol> <p>Good:</p> <p></p><pre><code>def test_parse_fasta_handles_multiline_sequences()\ndef test_alignment_score_returns_zero_for_empty_sequences()\ndef test_calculate_gc_content_raises_error_on_invalid_characters()\n</code></pre> Bad:<p></p> <p></p><pre><code>def test1()\ndef test_fasta()\ndef test_edge_case()\n</code></pre> 5. Test Both Success and Failure Don't just test the happy path. Test error conditions, edge cases, and boundary values.<p></p> <p></p><pre><code>def test_normalize_expression_positive_values():\n    \"\"\"Normal case: positive expression values.\"\"\"\n    result = normalize([1.0, 2.0, 3.0])\n    assert sum(result) == pytest.approx(1.0)\n\ndef test_normalize_expression_with_zeros():\n    \"\"\"Edge case: some zero values.\"\"\"\n    result = normalize([0.0, 1.0, 2.0])\n    assert result[0] == 0.0\n\ndef test_normalize_expression_all_zeros():\n    \"\"\"Failure case: cannot normalize all zeros.\"\"\"\n    with pytest.raises(ValueError):\n        normalize([0.0, 0.0, 0.0])\n</code></pre> 6. Avoid Testing Implementation Details Test the public interface and behavior, not internal implementation. This gives you freedom to refactor without rewriting tests.<p></p> <ol> <li>Make Failures Informative Use assertion messages or pytest's built-in failure output to make debugging easier.</li> </ol> <pre><code>def test_quality_threshold():\n    reads = filter_by_quality(fastq_data, min_quality=30)\n    assert len(reads) &gt; 0, f\"Expected filtered reads, got {len(reads)}\"\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#getting-started-a-practical-guide","title":"Getting Started: A Practical Guide","text":"<p>Installing pytest</p> <pre><code>pip install pytest\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#writing-your-first-test","title":"Writing Your First Test","text":"<p>Create a file starting with test_ (e.g., test_mycode.py). Write functions starting with test_:</p> <pre><code># test_mycode.py\ndef test_addition():\n    assert 1 + 1 == 2\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests in current directory\npytest\n\n# Run specific test file\npytest test_mycode.py\n\n# Run tests matching a pattern\npytest -k \"quality\"\n\n# Show print statements (useful for debugging)\npytest -s\n\n# Stop at first failure\npytest -x\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#structuring-your-project","title":"Structuring Your Project","text":"<pre><code>my_project/\n\u251c\u2500\u2500 mypackage/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 analysis.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_analysis.py\n\u2502   \u2514\u2500\u2500 test_utils.py\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 README.md\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#common-pitfalls-in-scientific-computing","title":"Common Pitfalls in Scientific Computing","text":"","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#floating-point-comparisons","title":"Floating Point Comparisons","text":"<p>Never use exact equality for floating-point numbers:</p> <p>Bad:</p> <pre><code>def test_mean():\n    assert calculate_mean([1.0, 2.0, 3.0]) == 2.0\n</code></pre> <p>Good:</p> <pre><code>def test_mean():\n    assert calculate_mean([1.0, 2.0, 3.0]) == pytest.approx(2.0)\n    # or\n    assert abs(calculate_mean([1.0, 2.0, 3.0]) - 2.0) &lt; 1e-10\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#random-number-generation","title":"Random Number Generation","text":"<p>Set seeds for reproducibility:</p> <pre><code>def test_random_sampling():\n    import random\n    random.seed(42)\n    sample = random_sample(population, size=10)\n    assert len(sample) == 10\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#file-io","title":"File I/O","text":"<p>Use temporary files or fixtures instead of relying on external files:</p> <pre><code>import tempfile\nimport pytest\n\n@pytest.fixture\ndef temp_fasta():\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:\n        f.write(\"&gt;seq1\\nATCG\\n\")\n        f.write(\"&gt;seq2\\nGCTA\\n\")\n        return f.name\n\ndef test_parse_fasta(temp_fasta):\n    sequences = parse_fasta(temp_fasta)\n    assert len(sequences) == 2\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#external-dependencies","title":"External Dependencies","text":"<p>Mock external services or databases:</p> <pre><code>from unittest.mock import Mock, patch\n\ndef test_fetch_gene_info():\n    with patch('mycode.ncbi_api.fetch') as mock_fetch:\n        mock_fetch.return_value = {'gene': 'BRCA1', 'chromosome': 17}\n        result = get_gene_location('BRCA1')\n        assert result['chromosome'] == 17\n</code></pre>","tags":["Testing","Python"]},{"location":"blog/unit-testing-for-research-software---a-practical-guide/#final-thoughts","title":"Final Thoughts","text":"<p>Unit testing isn't about achieving 100% code coverage or writing tests for the sake of writing tests. It's about building confidence in your code and making your research more reliable. Start small: pick one function that's been causing problems and write a few tests for it. Run them. See a bug get caught before it reaches production. You'll quickly see the value. Remember, every test you write is a bug you won't have to debug at midnight before a conference deadline. Your future self\u2014and your collaborators\u2014will thank you.</p> <p>Start with one test today. Your research deserves it.</p>","tags":["Testing","Python"]},{"location":"blog/version-control-with-git----a-technical-primer/","title":"Version Control with Git -  A Technical Primer","text":"<p>If you\u2019d like more information or hands-on training on Git and GitHub, please refer to this Introduction to Git and Github on KIR Training Catalogue</p> <p>Version control is the practice of tracking and managing changes to code over time. Git, created by Linus Torvalds in 2005, has become the dominant distributed version control system in software development.</p>","tags":["Git"]},{"location":"blog/version-control-with-git----a-technical-primer/#core-concepts","title":"Core Concepts","text":"<p>At its heart, Git maintains a directed acyclic graph (DAG) of commits. Each commit is a snapshot of your project at a specific point in time, identified by a SHA-1 hash. Unlike centralized systems like SVN, Git is distributed\u2014every developer has a complete copy of the repository history.</p> <p>The fundamental workflow involves three areas:</p> <ul> <li>Working directory: Your current files</li> <li>Staging area (index): Changes marked for the next commit</li> <li>Repository: Committed snapshots</li> </ul> <p>This staging area is Git's distinguishing feature, allowing you to craft precise commits by selectively adding changes.</p>","tags":["Git"]},{"location":"blog/version-control-with-git----a-technical-primer/#branching-and-merging","title":"Branching and Merging","text":"<p>Git's lightweight branching model is revolutionary. A branch is simply a movable pointer to a commit. Creating, switching, and merging branches is fast and cheap, encouraging workflows like feature branches and GitFlow.</p> <p>When merging, Git performs a three-way merge using the common ancestor of both branches. Conflicts arise when the same lines are modified differently, requiring manual resolution.</p>","tags":["Git"]},{"location":"blog/version-control-with-git----a-technical-primer/#practical-operations","title":"Practical Operations","text":"<p>Common operations include:</p> <ul> <li><code>git init</code> or <code>git clone</code> to start</li> <li><code>git add</code> to stage changes</li> <li><code>git commit</code> to snapshot</li> <li><code>git push/pull</code> to sync with remotes</li> <li><code>git branch</code> and <code>git merge</code> for parallel development</li> </ul> <p>Advanced features like rebase, cherry-pick, and interactive staging enable powerful history manipulation.</p>","tags":["Git"]},{"location":"blog/version-control-with-git----a-technical-primer/#why-git-matters","title":"Why Git Matters","text":"<p>Git enables collaborative development at scale. It provides complete history, enables experimentation through branching, and allows offline work. The GitHub/GitLab ecosystem built atop Git has transformed how software is developed and shared.</p> <p>Understanding Git's content-addressable storage model and object database\u2014where blobs, trees, commits, and tags form the foundation\u2014unlocks mastery of this essential tool.</p>","tags":["Git"]},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2026/#2026","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/category/programming/","title":"Programming","text":""},{"location":"blog/category/programming/#programming","title":"Programming","text":""},{"location":"blog/category/high-performance-computing/","title":"High Performance Computing","text":""},{"location":"blog/category/high-performance-computing/#high-performance-computing","title":"High Performance Computing","text":""},{"location":"blog/category/version-control/","title":"Version Control","text":""},{"location":"blog/category/version-control/#version-control","title":"Version Control","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/2/#blog","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/3/#blog","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/4/#blog","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/page/5/#blog","title":"Blog","text":""},{"location":"blog/page/6/","title":"Blog","text":""},{"location":"blog/page/6/#blog","title":"Blog","text":""},{"location":"blog/page/7/","title":"Blog","text":""},{"location":"blog/page/7/#blog","title":"Blog","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""},{"location":"blog/archive/2025/page/2/#2025","title":"2025","text":""},{"location":"blog/archive/2025/page/3/","title":"2025","text":""},{"location":"blog/archive/2025/page/3/#2025","title":"2025","text":""},{"location":"blog/archive/2025/page/4/","title":"2025","text":""},{"location":"blog/archive/2025/page/4/#2025","title":"2025","text":""},{"location":"blog/archive/2025/page/5/","title":"2025","text":""},{"location":"blog/archive/2025/page/5/#2025","title":"2025","text":""},{"location":"blog/archive/2025/page/6/","title":"2025","text":""},{"location":"blog/archive/2025/page/6/#2025","title":"2025","text":""},{"location":"blog/category/high-performance-computing/page/2/","title":"High Performance Computing","text":""},{"location":"blog/category/high-performance-computing/page/2/#high-performance-computing","title":"High Performance Computing","text":""},{"location":"blog/category/programming/page/2/","title":"Programming","text":""},{"location":"blog/category/programming/page/2/#programming","title":"Programming","text":""},{"location":"blog/category/programming/page/3/","title":"Programming","text":""},{"location":"blog/category/programming/page/3/#programming","title":"Programming","text":""},{"location":"blog/category/programming/page/4/","title":"Programming","text":""},{"location":"blog/category/programming/page/4/#programming","title":"Programming","text":""}]}